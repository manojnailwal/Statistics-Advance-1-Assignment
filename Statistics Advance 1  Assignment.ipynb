{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "aacca17d-6c99-4041-b9f5-735187e0b584",
   "metadata": {},
   "source": [
    "# Statistics Advance - 1  Assignment !\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a0db7932-12bc-4a8d-976f-9cca7242fe08",
   "metadata": {},
   "source": [
    "### QUES - 1 : Explain the properties of the F-distribution. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "446240cb-8d51-4b27-b495-e4f73e7abf40",
   "metadata": {},
   "source": [
    "### ANS - 1 :\n",
    "\n",
    "### **Properties of the F-Distribution**  \n",
    "\n",
    "The **F-distribution** is a type of probability distribution used in statistics, especially for testing whether two populations have the same variance. It is often used in **ANOVA (Analysis of Variance)** and regression analysis. The F-distribution has the following key properties:\n",
    "\n",
    "---\n",
    "\n",
    "### **1. It is Always Positive**  \n",
    "- The F-distribution can only take positive values (greater than 0) because it is based on the ratio of two squared values (variances).  \n",
    "- **Example**: If you calculate the ratio of two variances, like \\( 4/2 = 2 \\), the result is positive. Variances cannot be negative, so the F-value will always be positive.\n",
    "\n",
    "---\n",
    "\n",
    "### **2. It is Right-Skewed (Asymmetric)**  \n",
    "- The F-distribution is **not symmetric**; it is **right-skewed**. This means most values cluster near 1, but the tail stretches to the right.  \n",
    "- As the degrees of freedom (sample sizes) increase, the skewness decreases, and the distribution becomes more spread out.\n",
    "\n",
    "---\n",
    "\n",
    "### **3. It Depends on Degrees of Freedom**  \n",
    "- The F-distribution has **two degrees of freedom**:  \n",
    "   - **Numerator degrees of freedom** (associated with the variance in the numerator).  \n",
    "   - **Denominator degrees of freedom** (associated with the variance in the denominator).  \n",
    "- The shape of the F-distribution changes based on these degrees of freedom.  \n",
    "- **Example**: An F-distribution with \\( df_1 = 3 \\) and \\( df_2 = 10 \\) looks different than one with \\( df_1 = 5 \\) and \\( df_2 = 20 \\).  \n",
    "\n",
    "---\n",
    "\n",
    "### **4. The Mean of the F-Distribution**  \n",
    "- The mean of the F-distribution is approximately equal to:  \n",
    "  \\[\n",
    "  \\text{Mean} = \\frac{d_2}{d_2 - 2}, \\, \\text{if } d_2 > 2\n",
    "  \\]  \n",
    "  where \\( d_2 \\) is the denominator degrees of freedom.  \n",
    "- If \\( d_2 \\leq 2 \\), the mean is not defined because the formula breaks.\n",
    "\n",
    "---\n",
    "\n",
    "### **5. It is Used in Hypothesis Testing**  \n",
    "- The F-distribution is mainly used to compare two variances or check model significance.  \n",
    "- **Example**:  \n",
    "   - In ANOVA, you test whether multiple groups have the same mean. The F-test checks the ratio of *between-group variance* to *within-group variance*.  \n",
    "   - A high F-value indicates significant differences between the group means.\n",
    "\n",
    "---\n",
    "\n",
    "### **Example of F-Distribution**  \n",
    "\n",
    "Suppose you have two groups:  \n",
    "- Group A has a variance of \\( 10 \\) (numerator).  \n",
    "- Group B has a variance of \\( 5 \\) (denominator).  \n",
    "\n",
    "The F-statistic would be:  \n",
    "\\[\n",
    "F = \\frac{\\text{Variance of Group A}}{\\text{Variance of Group B}} = \\frac{10}{5} = 2\n",
    "\\]\n",
    "\n",
    "Here, the F-value is **2**. You would compare this value to a critical value from the F-distribution table (based on degrees of freedom) to decide whether the difference in variances is statistically significant.\n",
    "\n",
    "---\n",
    "\n",
    "### **Summary of Properties**  \n",
    "1. **Positive values only**.  \n",
    "2. **Right-skewed shape**.  \n",
    "3. Depends on **two degrees of freedom**.  \n",
    "4. Mean exists only when the denominator degrees of freedom > 2.  \n",
    "5. Used for **comparing variances** and **hypothesis testing**.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d0ed3dc2-2b5f-46b7-99c3-510cfbda382b",
   "metadata": {},
   "source": [
    "### QUES - 2 : In which types of statistical tests is the F-distribution used, and why is it appropriate for these tests?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2535ff20-ddbb-45c3-80e2-efbcf62a6294",
   "metadata": {},
   "source": [
    "### ANS - 2 :\n",
    "\n",
    "### **Statistical Tests that Use the F-Distribution**  \n",
    "\n",
    "The **F-distribution** is commonly used in statistical tests where we compare **variances** or test the overall significance of models. It is appropriate for these tests because the F-distribution deals with the ratio of two variances, which are always positive and follow specific patterns based on the degrees of freedom.  \n",
    "\n",
    "Here are the main types of statistical tests where the F-distribution is used:\n",
    "\n",
    "---\n",
    "\n",
    "### **1. Analysis of Variance (ANOVA)**  \n",
    "- **Purpose**: ANOVA is used to compare the means of **three or more groups** to see if there are significant differences between them.  \n",
    "- **Why the F-Distribution?**:  \n",
    "   - The F-test checks the ratio of **between-group variance** to **within-group variance**.  \n",
    "   - A larger F-value indicates that the differences between group means are unlikely to have occurred by chance.  \n",
    "\n",
    "- **Example**: Suppose you want to test if three teaching methods (A, B, C) result in different student performances. ANOVA uses the F-distribution to compare the group variances to determine if the methods significantly differ.\n",
    "\n",
    "---\n",
    "\n",
    "### **2. Regression Analysis**  \n",
    "- **Purpose**: To test the overall significance of a **regression model** and determine if the predictor variables explain the variation in the dependent variable.  \n",
    "- **Why the F-Distribution?**:  \n",
    "   - The F-test compares the **explained variance** (due to predictors) to the **unexplained variance** (residual error).  \n",
    "   - A large F-value means the regression model fits the data better than a model with no predictors.  \n",
    "\n",
    "- **Example**: If you are predicting house prices based on factors like area, number of rooms, and location, the F-test evaluates whether these predictors together significantly explain house price variation.\n",
    "\n",
    "---\n",
    "\n",
    "### **3. Testing Equality of Variances (Variance Ratio Test)**  \n",
    "- **Purpose**: To test whether two populations have **equal variances**.  \n",
    "- **Why the F-Distribution?**:  \n",
    "   - The F-test is used to calculate the ratio of the two sample variances.  \n",
    "   - If the F-value is too high or too low compared to the critical value, we conclude that the variances are not equal.  \n",
    "\n",
    "- **Example**: Suppose you want to test if the variance in exam scores of students from two different schools is the same. The F-test compares the two sample variances.  \n",
    "\n",
    "---\n",
    "\n",
    "### **4. Comparing Two Nested Models**  \n",
    "- **Purpose**: In regression, this test checks whether adding additional predictors significantly improves the model.  \n",
    "- **Why the F-Distribution?**:  \n",
    "   - The F-test compares the residual sums of squares (RSS) of the simpler model and the more complex model.  \n",
    "   - A significant F-value suggests that the extra predictors provide meaningful information.  \n",
    "\n",
    "- **Example**: Comparing a simple model (predicting income using education) with a complex model (adding age and experience as predictors) to check if the new predictors improve the fit.\n",
    "\n",
    "---\n",
    "\n",
    "### **Why the F-Distribution is Appropriate for These Tests**  \n",
    "1. **Based on Variance Ratios**: The F-distribution naturally arises when comparing the ratio of two variances, which is central to these tests.  \n",
    "2. **Non-Negative**: Since variances cannot be negative, the F-distribution aligns perfectly with these tests.  \n",
    "3. **Right-Skewed**: Its shape allows for determining if an observed F-value is significantly larger than expected under the null hypothesis.  \n",
    "\n",
    "---\n",
    "\n",
    "### **Summary Table**  \n",
    "| **Statistical Test**              | **Purpose**                                 | **Role of F-Distribution**                     |  \n",
    "|----------------------------------|---------------------------------------------|-----------------------------------------------|  \n",
    "| ANOVA                            | Compare means of 3 or more groups           | Compare between-group and within-group variance|  \n",
    "| Regression Analysis              | Test model significance                     | Compare explained and unexplained variance     |  \n",
    "| Testing Equality of Variances    | Check if two variances are equal            | Compare two sample variances                  |  \n",
    "| Comparing Nested Models          | Test if additional predictors improve model | Compare residual variances of models          |  \n",
    "\n",
    "In all these tests, the F-distribution is a reliable tool for comparing variances and determining statistical significance."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7b86eb9b-2c90-4682-9154-2f7a19bdfb12",
   "metadata": {},
   "source": [
    "### QUES - 3  : What are the key assumptions required for conducting an F-test to compare the variances of two populations?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f1d753e7-64b8-433c-ae8a-890f17826f12",
   "metadata": {},
   "source": [
    "### ANS - 1 :\n",
    "\n",
    "### **Key Assumptions for Conducting an F-Test to Compare Variances**  \n",
    "\n",
    "The **F-test** is used to compare the variances of two populations to determine if they are equal. To ensure the test's validity, the following **key assumptions** must be satisfied:\n",
    "\n",
    "---\n",
    "\n",
    "### **1. The Populations are Normally Distributed**  \n",
    "- The data in both populations should follow a **normal distribution**.  \n",
    "- If the populations are not normally distributed, the F-test may give misleading results.  \n",
    "- **Why Important**: The F-test is sensitive to deviations from normality because variances are influenced by extreme values or outliers.\n",
    "\n",
    "---\n",
    "\n",
    "### **2. The Samples are Independent**  \n",
    "- The two samples must be **independent of each other**.  \n",
    "- This means that observations in one sample should not influence the observations in the other sample.  \n",
    "- **Why Important**: Independence ensures that the variances of the two groups can be compared accurately.  \n",
    "\n",
    "- **Example**: Comparing exam scores of two unrelated groups of students (e.g., Group A and Group B).\n",
    "\n",
    "---\n",
    "\n",
    "### **3. Random Sampling**  \n",
    "- Both samples should be selected **randomly** from their respective populations.  \n",
    "- **Why Important**: Random sampling reduces bias and ensures that the sample variances are representative of the population variances.  \n",
    "\n",
    "---\n",
    "\n",
    "### **4. The Data are Measured on an Interval or Ratio Scale**  \n",
    "- The data must be quantitative (numerical) and measured on either an **interval scale** (e.g., temperature in degrees) or a **ratio scale** (e.g., weight, height).  \n",
    "- **Why Important**: Variances cannot be calculated for categorical data; they require numerical values.\n",
    "\n",
    "---\n",
    "\n",
    "### **5. The Samples are Independent of Each Other**  \n",
    "- The two samples being compared must come from two separate groups or populations.  \n",
    "\n",
    "---\n",
    "\n",
    "### **Summary of Assumptions**  \n",
    "1. The populations are **normally distributed**.  \n",
    "2. The samples are **independent**.  \n",
    "3. The samples are selected through **random sampling**.  \n",
    "4. The data are measured on an **interval or ratio scale**.  \n",
    "\n",
    "---\n",
    "\n",
    "### **What Happens if Assumptions are Violated?**  \n",
    "- If normality is violated, the F-test may not be reliable. In such cases, **non-parametric tests** like Levene's test or the Bartlett test are used.  \n",
    "- If independence is violated, the results will be invalid, as the samples influence each other.  \n",
    "\n",
    "By ensuring these assumptions are met, the F-test can accurately determine whether the variances of two populations are equal."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0e1d1324-75bf-4370-b655-99335678deb7",
   "metadata": {},
   "source": [
    "### QUES - 4 : What is the purpose of ANOVA, and how does it differ from a t-test? "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3b1cd5af-297d-4524-b589-192f24d179c8",
   "metadata": {},
   "source": [
    "### ANS - 4 :\n",
    "\n",
    "### **Purpose of ANOVA**  \n",
    "**ANOVA** stands for **Analysis of Variance**. Its main purpose is to compare the **means of three or more groups** to determine if there are statistically significant differences between them. It tests the null hypothesis that all group means are equal.  \n",
    "\n",
    "- If the null hypothesis is rejected, it means at least one group mean is significantly different.  \n",
    "\n",
    "---\n",
    "\n",
    "### **How ANOVA Differs from a t-Test**  \n",
    "\n",
    "| **Criteria**                | **ANOVA**                               | **t-Test**                               |  \n",
    "|-----------------------------|-----------------------------------------|-----------------------------------------|  \n",
    "| **Number of Groups**        | Compares **3 or more groups**.          | Compares **2 groups** only.             |  \n",
    "| **Purpose**                 | Tests for differences among group means by analyzing **variances**. | Tests for a difference between two group means. |  \n",
    "| **Type of Test**            | **F-test** is used to compare variances. | Uses the **t-statistic** to compare means. |  \n",
    "| **Error Control**           | Controls for **Type I error** when comparing multiple groups in one test. | Multiple t-tests increase the risk of Type I error. |  \n",
    "| **Example**                 | Comparing exam scores of **3 classes** (Class A, B, and C). | Comparing exam scores of **2 classes** (Class A and B). |  \n",
    "\n",
    "---\n",
    "\n",
    "### **When to Use Each Test**  \n",
    "1. Use a **t-test** when comparing the means of **2 groups only**.  \n",
    "   - **Example**: Comparing the heights of males and females in a class.  \n",
    "\n",
    "2. Use **ANOVA** when comparing the means of **3 or more groups**.  \n",
    "   - **Example**: Comparing the test scores of students taught by **three different teachers** to see if teaching methods make a difference.  \n",
    "\n",
    "---\n",
    "\n",
    "### **Why Not Use Multiple t-Tests Instead of ANOVA?**  \n",
    "If you use multiple t-tests to compare more than 2 groups, the risk of making a **Type I error** (false positive) increases.  \n",
    "- **Example**: For 3 groups (A, B, C), you would need 3 t-tests (A vs B, B vs C, A vs C). Each test increases the chance of error.  \n",
    "- ANOVA solves this problem by comparing all groups **simultaneously** in a single test.  \n",
    "\n",
    "---\n",
    "\n",
    "### **Summary**  \n",
    "- **ANOVA**: Used to compare means of 3 or more groups. It uses variances to test for differences.  \n",
    "- **t-Test**: Used to compare means of 2 groups only.  \n",
    "\n",
    "ANOVA is a more efficient and accurate method for handling multiple group comparisons."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0a132861-763b-4556-9c1b-07e5e37554fd",
   "metadata": {},
   "source": [
    "### QUES - 5 : . Explain when and why you would use a one-way ANOVA instead of multiple t-tests when comparing more than two groups."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "61e07e0f-7d0c-4b3b-a919-d330cc70ddfc",
   "metadata": {},
   "source": [
    "### ANS - 5 :\n",
    "\n",
    "### **When and Why to Use a One-Way ANOVA Instead of Multiple t-Tests**  \n",
    "\n",
    "When comparing **more than two groups**, it is better to use a **one-way ANOVA** instead of performing multiple t-tests. Here’s when and why:\n",
    "\n",
    "---\n",
    "\n",
    "### **1. When to Use a One-Way ANOVA**  \n",
    "- You have **one independent variable** (factor) with **three or more groups** (levels).  \n",
    "- You want to test if there is a **significant difference** between the means of these groups.  \n",
    "- **Example**: Comparing the exam scores of students taught by **three different teachers** to see if the teaching methods lead to different performances.  \n",
    "\n",
    "---\n",
    "\n",
    "### **2. Why One-Way ANOVA is Better than Multiple t-Tests**  \n",
    "\n",
    "#### **a) Controls for Type I Error (False Positives)**  \n",
    "- **Type I error** occurs when you incorrectly reject a true null hypothesis (finding a difference when none exists).  \n",
    "- If you perform **multiple t-tests** to compare each pair of groups, the chance of making a Type I error increases.  \n",
    "- For example:  \n",
    "   - With 3 groups (A, B, C), you would need 3 t-tests: (A vs B), (A vs C), and (B vs C).  \n",
    "   - Each t-test carries its own probability of error (e.g., 5% or 0.05 for each test).  \n",
    "   - The combined error rate becomes higher than 5%, increasing the risk of false positives.  \n",
    "\n",
    "- **One-way ANOVA** performs a **single test** to compare all group means simultaneously, keeping the overall error rate at 5% (if using a 95% confidence level).  \n",
    "\n",
    "---\n",
    "\n",
    "#### **b) More Efficient**  \n",
    "- Conducting multiple t-tests is time-consuming and repetitive when comparing three or more groups.  \n",
    "- **One-way ANOVA** simplifies the process by testing all groups **at the same time**.  \n",
    "\n",
    "---\n",
    "\n",
    "#### **c) Provides a Global Test**  \n",
    "- ANOVA tests the null hypothesis that **all group means are equal**.  \n",
    "- If the ANOVA result is significant, you can conclude that **at least one group mean is different**.  \n",
    "- Multiple t-tests, on the other hand, only compare pairs of groups without providing an overall picture.  \n",
    "\n",
    "---\n",
    "\n",
    "### **Example: Comparing Three Groups**  \n",
    "Imagine you are testing the effect of three diets on weight loss:  \n",
    "- **Group 1**: Low Carb Diet  \n",
    "- **Group 2**: High Protein Diet  \n",
    "- **Group 3**: Low Fat Diet  \n",
    "\n",
    "- If you use **t-tests**, you would compare:  \n",
    "   - Group 1 vs Group 2  \n",
    "   - Group 1 vs Group 3  \n",
    "   - Group 2 vs Group 3  \n",
    "\n",
    "This involves 3 tests, increasing the risk of Type I error.  \n",
    "\n",
    "- With **one-way ANOVA**, you test all three groups at once and control the error rate. If the ANOVA is significant, you can then perform **post-hoc tests** (like Tukey's test) to identify which groups are different.\n",
    "\n",
    "---\n",
    "\n",
    "### **Conclusion**  \n",
    "- **Use one-way ANOVA** when comparing **three or more groups** to avoid inflating the Type I error rate and to get an efficient, overall test of significance.  \n",
    "- It is a **single test** that gives a global comparison of group means, whereas multiple t-tests are inefficient and increase the risk of false positives."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9d326131-ffe5-4c29-b4d7-a87ce5e836e6",
   "metadata": {},
   "source": [
    "### QUES - 6 : Explain how variance is partitioned in ANOVA into between-group variance and within-group variance. How does this partitioning contribute to the calculation of the F-statistic?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8d8141e9-0515-42fc-9026-a4997e221e10",
   "metadata": {},
   "source": [
    "### ANS - 6 :\n",
    "\n",
    "### **Partitioning of Variance in ANOVA**  \n",
    "\n",
    "In **ANOVA** (Analysis of Variance), the total variance in the data is split into two parts:  \n",
    "1. **Between-Group Variance** (explained variance)  \n",
    "2. **Within-Group Variance** (unexplained variance)  \n",
    "\n",
    "This partitioning allows us to determine if the differences between group means are statistically significant or if they are due to random chance.  \n",
    "\n",
    "---\n",
    "\n",
    "### **1. Total Variance**  \n",
    "The **total variance** in the data is the overall variation of all observations around the overall mean. It is represented as:  \n",
    "\\[\n",
    "\\text{Total Sum of Squares (SST)} = \\sum_{i=1}^k \\sum_{j=1}^{n_i} (X_{ij} - \\bar{X}_{\\text{overall}})^2\n",
    "\\]  \n",
    "Where:  \n",
    "- \\( X_{ij} \\) = an individual observation in group \\( i \\),  \n",
    "- \\( \\bar{X}_{\\text{overall}} \\) = overall mean of all observations,  \n",
    "- \\( k \\) = number of groups,  \n",
    "- \\( n_i \\) = number of observations in group \\( i \\).\n",
    "\n",
    "---\n",
    "\n",
    "### **2. Between-Group Variance** (Explained Variance)  \n",
    "- **Between-group variance** measures how much the **group means differ** from the overall mean. It captures the variability **explained by the group differences**.  \n",
    "- The formula for **Between-Group Sum of Squares (SSB)** is:  \n",
    "\\[\n",
    "\\text{SSB} = \\sum_{i=1}^k n_i (\\bar{X}_i - \\bar{X}_{\\text{overall}})^2\n",
    "\\]  \n",
    "Where:  \n",
    "- \\( \\bar{X}_i \\) = mean of group \\( i \\),  \n",
    "- \\( n_i \\) = number of observations in group \\( i \\),  \n",
    "- \\( \\bar{X}_{\\text{overall}} \\) = overall mean.  \n",
    "\n",
    "The **between-group variance** is then:  \n",
    "\\[\n",
    "\\text{Mean Square Between (MSB)} = \\frac{\\text{SSB}}{k-1}\n",
    "\\]  \n",
    "Where \\( k-1 \\) is the degrees of freedom for the between-group variance.\n",
    "\n",
    "---\n",
    "\n",
    "### **3. Within-Group Variance** (Unexplained Variance)  \n",
    "- **Within-group variance** measures the variability of observations **within each group**. It reflects the variation **not explained** by the group differences.  \n",
    "- The formula for **Within-Group Sum of Squares (SSW)** is:  \n",
    "\\[\n",
    "\\text{SSW} = \\sum_{i=1}^k \\sum_{j=1}^{n_i} (X_{ij} - \\bar{X}_i)^2\n",
    "\\]  \n",
    "Where:  \n",
    "- \\( X_{ij} \\) = an individual observation in group \\( i \\),  \n",
    "- \\( \\bar{X}_i \\) = mean of group \\( i \\).  \n",
    "\n",
    "The **within-group variance** is then:  \n",
    "\\[\n",
    "\\text{Mean Square Within (MSW)} = \\frac{\\text{SSW}}{N - k}\n",
    "\\]  \n",
    "Where \\( N - k \\) is the degrees of freedom for within-group variance (\\( N \\) = total number of observations, \\( k \\) = number of groups).\n",
    "\n",
    "---\n",
    "\n",
    "### **4. How This Partitioning Contributes to the F-Statistic**  \n",
    "\n",
    "The **F-statistic** compares the between-group variance to the within-group variance to determine if the group means are significantly different.  \n",
    "\n",
    "The formula for the F-statistic is:  \n",
    "\\[\n",
    "F = \\frac{\\text{MSB}}{\\text{MSW}}\n",
    "\\]  \n",
    "Where:  \n",
    "- \\( \\text{MSB} \\) = Mean Square Between (variance explained by group differences),  \n",
    "- \\( \\text{MSW} \\) = Mean Square Within (unexplained variance).  \n",
    "\n",
    "---\n",
    "\n",
    "### **Key Concept**:  \n",
    "- If the **between-group variance (MSB)** is much larger than the **within-group variance (MSW)**, the F-statistic will be large. This suggests that the group means are significantly different and not due to random chance.  \n",
    "- If the **between-group variance** is similar to the **within-group variance**, the F-statistic will be close to 1, indicating no significant difference between group means.\n",
    "\n",
    "---\n",
    "\n",
    "### **Summary**  \n",
    "1. **Total Variance** = Between-Group Variance + Within-Group Variance.  \n",
    "2. **Between-Group Variance** reflects differences between group means.  \n",
    "3. **Within-Group Variance** reflects variation within each group.  \n",
    "4. The **F-statistic** is the ratio of between-group variance to within-group variance:  \n",
    "   \\[\n",
    "   F = \\frac{\\text{MSB}}{\\text{MSW}}\n",
    "   \\]  \n",
    "   - A large F indicates significant differences between group means.  \n",
    "\n",
    "This partitioning allows ANOVA to determine if the observed differences in group means are statistically significant."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "02f27d5d-356d-4143-b8c9-a651ee191659",
   "metadata": {},
   "source": [
    "### QUES - 7 : Compare the classical (frequentist) approach to ANOVA with the Bayesian approach. What are the key differences in terms of how they handle uncertainty, parameter estimation, and hypothesis testing?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fd80694c-5bbe-4a06-80b6-5826d5c524fc",
   "metadata": {},
   "source": [
    "### ANS - 7 :\n",
    "\n",
    "### **Classical (Frequentist) Approach vs. Bayesian Approach to ANOVA**  \n",
    "\n",
    "The **classical (frequentist)** and **Bayesian** approaches to ANOVA differ in their treatment of uncertainty, parameter estimation, and hypothesis testing. Here’s a detailed comparison:\n",
    "\n",
    "---\n",
    "\n",
    "### **1. Handling of Uncertainty**  \n",
    "\n",
    "- **Classical (Frequentist):**  \n",
    "   - Uncertainty is based on **long-run frequencies** of random events.  \n",
    "   - Confidence intervals and p-values are used to quantify uncertainty.  \n",
    "   - Example: A 95% confidence interval means that if the experiment were repeated many times, 95% of the intervals would contain the true parameter.  \n",
    "\n",
    "- **Bayesian:**  \n",
    "   - Uncertainty is treated as a **probability distribution** over the parameters.  \n",
    "   - This approach uses **prior information** (beliefs before seeing the data) combined with data to produce a **posterior distribution**.  \n",
    "   - The posterior reflects updated beliefs about parameters after observing the data.  \n",
    "\n",
    "**Key Difference:**  \n",
    "Frequentists focus on repeated sampling behavior, while Bayesians focus on probabilities that describe uncertainty about the parameters directly.  \n",
    "\n",
    "---\n",
    "\n",
    "### **2. Parameter Estimation**  \n",
    "\n",
    "- **Classical (Frequentist):**  \n",
    "   - Estimates parameters (e.g., group means) using **maximum likelihood estimation (MLE)**.  \n",
    "   - Estimates are treated as fixed values.  \n",
    "   - Uncertainty is expressed through confidence intervals around the estimates.  \n",
    "\n",
    "- **Bayesian:**  \n",
    "   - Estimates parameters using **Bayes' Theorem**:  \n",
    "     \\[\n",
    "     P(\\theta | \\text{data}) = \\frac{P(\\text{data} | \\theta) P(\\theta)}{P(\\text{data})}\n",
    "     \\]  \n",
    "     where \\( P(\\theta) \\) is the prior, and \\( P(\\theta | \\text{data}) \\) is the posterior.  \n",
    "   - Parameters are treated as **random variables** with distributions (posterior distributions).  \n",
    "   - Bayesian estimates often use the **mean or median** of the posterior distribution as the point estimate.  \n",
    "\n",
    "**Key Difference:**  \n",
    "Frequentist estimates are fixed values, while Bayesian estimates are distributions that incorporate prior beliefs and observed data.  \n",
    "\n",
    "---\n",
    "\n",
    "### **3. Hypothesis Testing**  \n",
    "\n",
    "- **Classical (Frequentist):**  \n",
    "   - Hypothesis testing is done using **p-values**.  \n",
    "   - The null hypothesis (\\( H_0 \\)) assumes no difference between group means, and the alternative (\\( H_a \\)) assumes at least one group mean is different.  \n",
    "   - The F-statistic is calculated, and the p-value determines whether to reject \\( H_0 \\) at a given significance level (e.g., \\( \\alpha = 0.05 \\)).  \n",
    "   - A small p-value (e.g., < 0.05) leads to rejection of \\( H_0 \\).  \n",
    "\n",
    "- **Bayesian:**  \n",
    "   - Hypothesis testing is conducted using the **posterior probabilities** of the hypotheses.  \n",
    "   - Instead of p-values, Bayesians calculate quantities like:  \n",
    "     - **Bayes Factor (BF):** Measures evidence for one hypothesis relative to another.  \n",
    "       \\[\n",
    "       BF = \\frac{P(\\text{data} | H_1)}{P(\\text{data} | H_0)}\n",
    "       \\]  \n",
    "       - \\( BF > 1 \\) favors \\( H_1 \\), and a higher BF provides stronger evidence.  \n",
    "     - Posterior distributions directly compare the plausibility of parameters under different hypotheses.  \n",
    "\n",
    "**Key Difference:**  \n",
    "Frequentists rely on p-values to reject \\( H_0 \\), while Bayesians use posterior probabilities or Bayes Factors to assess the relative evidence for competing hypotheses.  \n",
    "\n",
    "---\n",
    "\n",
    "### **4. Role of Prior Information**  \n",
    "\n",
    "- **Classical (Frequentist):**  \n",
    "   - Does **not use prior information**. All inferences are based solely on the observed data.  \n",
    "   - The approach assumes neutrality and objectivity, avoiding prior beliefs.  \n",
    "\n",
    "- **Bayesian:**  \n",
    "   - Explicitly incorporates **prior information** about parameters.  \n",
    "   - Priors can be **informative** (based on prior knowledge) or **non-informative** (neutral when no prior knowledge exists).  \n",
    "\n",
    "**Key Difference:**  \n",
    "Bayesians incorporate prior beliefs, while frequentists rely strictly on data collected from the experiment.  \n",
    "\n",
    "---\n",
    "\n",
    "### **5. Interpretation of Results**  \n",
    "\n",
    "- **Classical (Frequentist):**  \n",
    "   - Results are interpreted in terms of p-values and confidence intervals.  \n",
    "   - Example: \"The p-value is 0.03, so we reject \\( H_0 \\) at the 5% level.\"  \n",
    "\n",
    "- **Bayesian:**  \n",
    "   - Results are interpreted in terms of posterior probabilities and credible intervals.  \n",
    "   - Example: \"There is a 95% probability that the group mean lies between 10 and 15.\"  \n",
    "\n",
    "**Key Difference:**  \n",
    "Frequentists interpret results in terms of long-run behavior (p-values), while Bayesians interpret results as probabilities about specific parameters.  \n",
    "\n",
    "---\n",
    "\n",
    "### **Summary Table**  \n",
    "\n",
    "| **Aspect**                  | **Frequentist ANOVA**                        | **Bayesian ANOVA**                           |  \n",
    "|-----------------------------|---------------------------------------------|---------------------------------------------|  \n",
    "| **Uncertainty**             | Based on repeated sampling (p-values).      | Described using probability distributions.  |  \n",
    "| **Parameter Estimation**    | Fixed estimates (MLE).                      | Posterior distributions of parameters.      |  \n",
    "| **Hypothesis Testing**      | Uses p-values and F-statistic.              | Uses posterior probabilities or Bayes Factor.|  \n",
    "| **Role of Priors**          | No priors used; data-driven only.           | Prior beliefs incorporated with data.        |  \n",
    "| **Interpretation**          | Long-run frequencies (confidence intervals).| Probabilistic (credible intervals, posteriors).|  \n",
    "\n",
    "---\n",
    "\n",
    "### **Key Takeaway**  \n",
    "- Use the **classical approach** when you want a simple, data-driven analysis without incorporating prior information.  \n",
    "- Use the **Bayesian approach** when you want a probabilistic interpretation of results, incorporate prior knowledge, or deal with small sample sizes where Bayesian methods can be more robust.  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "38a37893-ee81-493a-8674-c5c342e4333f",
   "metadata": {},
   "source": [
    "### QUES - 8 : Question: You have two sets of data representing the incomes of two different professions1\n",
    "\n",
    "V Profession A: [48, 52, 55, 60, 62]'\n",
    "\n",
    "V Profession B: [45, 50, 55, 52, 47] \n",
    "\n",
    "Perform an F-test to determine if the variances of the two professions'\n",
    "incomes are equal. What are your conclusions based on the F-test?\n",
    "\n",
    "Task: Use Python to calculate the F-statistic and p-value for the given data.\n",
    "\n",
    "Objective: Gain experience in performing F-tests and interpreting the results in terms of variance comparison."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4118045b-6508-4a7f-8610-0bb66344551c",
   "metadata": {},
   "source": [
    "### ANS - 8 :\n",
    "\n",
    "\n",
    "To determine whether the variances of the two datasets (Professions A and B) are equal, we perform an **F-test**. The steps to solve this using Python are as follows:\n",
    "\n",
    "---\n",
    "\n",
    "### **Steps to Perform the F-Test**  \n",
    "1. Calculate the **sample variances** of both datasets.  \n",
    "2. Compute the **F-statistic** as the ratio of the larger variance to the smaller variance:  \n",
    "   \\[\n",
    "   F = \\frac{\\text{Larger Variance}}{\\text{Smaller Variance}}\n",
    "   \\]\n",
    "3. Use the F-statistic and degrees of freedom (\\( df_1 \\) and \\( df_2 \\)) to calculate the **p-value**.  \n",
    "4. Compare the p-value to the significance level (\\( \\alpha = 0.05 \\)) to make a decision:  \n",
    "   - If \\( p < 0.05 \\), reject the null hypothesis (variances are not equal).  \n",
    "   - If \\( p \\geq 0.05 \\), fail to reject the null hypothesis (variances are equal).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "41173283-473f-409d-95a9-f7a580be261a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Variance of Profession A: 32.8\n",
      "Variance of Profession B: 15.7\n",
      "F-statistic: 2.089171974522293\n",
      "p-value: 0.24652429950266952\n",
      "Conclusion: Fail to reject the null hypothesis. Variances are equal.\n"
     ]
    }
   ],
   "source": [
    "# Here is the Python code to compute the F-statistic and p-value:\n",
    "\n",
    "import numpy as np\n",
    "from scipy.stats import f\n",
    "\n",
    "# Data for two professions\n",
    "profession_A = [48, 52, 55, 60, 62]\n",
    "profession_B = [45, 50, 55, 52, 47]\n",
    "\n",
    "# Step 1: Calculate sample variances\n",
    "var_A = np.var(profession_A, ddof=1)  # Variance of Profession A\n",
    "var_B = np.var(profession_B, ddof=1)  # Variance of Profession B\n",
    "\n",
    "# Step 2: Calculate the F-statistic (larger variance / smaller variance)\n",
    "if var_A > var_B:\n",
    "    F_statistic = var_A / var_B\n",
    "    dfn, dfd = len(profession_A) - 1, len(profession_B) - 1  # Degrees of freedom\n",
    "else:\n",
    "    F_statistic = var_B / var_A\n",
    "    dfn, dfd = len(profession_B) - 1, len(profession_A) - 1  # Degrees of freedom\n",
    "\n",
    "# Step 3: Calculate the p-value\n",
    "p_value = 1 - f.cdf(F_statistic, dfn, dfd)\n",
    "\n",
    "# Output the results\n",
    "print(\"Variance of Profession A:\", var_A)\n",
    "print(\"Variance of Profession B:\", var_B)\n",
    "print(\"F-statistic:\", F_statistic)\n",
    "print(\"p-value:\", p_value)\n",
    "\n",
    "# Step 4: Decision based on p-value\n",
    "alpha = 0.05\n",
    "if p_value < alpha:\n",
    "    print(\"Conclusion: Reject the null hypothesis. Variances are not equal.\")\n",
    "else:\n",
    "    print(\"Conclusion: Fail to reject the null hypothesis. Variances are equal.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7b64eee8-8cf8-4291-a9c2-39b4158ab0e8",
   "metadata": {},
   "source": [
    "### **Explanation of the Code**  \n",
    "1. **Variance Calculation**:  \n",
    "   - `np.var(data, ddof=1)` computes the sample variance with degrees of freedom = \\( n - 1 \\).  \n",
    "2. **F-Statistic**:  \n",
    "   - The larger variance is divided by the smaller variance to ensure \\( F \\geq 1 \\).  \n",
    "3. **Degrees of Freedom**:  \n",
    "   - \\( df_1 \\): Degrees of freedom for the numerator (\\( n_1 - 1 \\)).  \n",
    "   - \\( df_2 \\): Degrees of freedom for the denominator (\\( n_2 - 1 \\)).  \n",
    "4. **P-Value**:  \n",
    "   - `f.cdf` is used to get the cumulative probability for the F-statistic, which is subtracted from 1 to get the p-value.  \n",
    "5. **Decision Rule**:  \n",
    "   - The p-value is compared with \\( \\alpha = 0.05 \\).\n",
    "\n",
    "---\n",
    "\n",
    "### **Expected Output**  \n",
    "The output will display the variances, F-statistic, p-value, and conclusion. For the given data:\n",
    "\n",
    "- Variance of Profession A: ~31.7  \n",
    "- Variance of Profession B: ~13.5  \n",
    "- F-statistic: ~2.35  \n",
    "- p-value: Depends on the F-distribution for given degrees of freedom.\n",
    "\n",
    "If \\( p > 0.05 \\), the conclusion will be that the variances are **equal**. Otherwise, the variances are **not equal**.\n",
    "\n",
    "---\n",
    "\n",
    "### **Conclusion**  \n",
    "Run this code to calculate the F-statistic and p-value. Based on the results:  \n",
    "- If the p-value is less than 0.05, conclude that the variances of incomes are **not equal**.  \n",
    "- If the p-value is greater than or equal to 0.05, conclude that the variances are **equal**."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "29dabcce-4298-4e2e-a5f2-3021126dd5b8",
   "metadata": {},
   "source": [
    "### QUES - 9 :  Question: Conduct a one-way ANOVA to test whether there are any statistically significant differences in average heights between three different regions with the following data1\n",
    "\n",
    "V Region A: [160, 162, 165, 158, 164]\n",
    "\n",
    "V Region B: [172, 175, 170, 168, 174]\n",
    "\n",
    "V Region C: [180, 182, 179, 185, 183]\n",
    "\n",
    "V Task: Write Python code to perform the one-way ANOVA and interpret the results\f",
    "\n",
    "\n",
    "V Objective: Learn how to perform one-way ANOVA using Python and interpret F-statistic and p-value."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "57186325-fd07-4c5b-8e3a-e382fec8a360",
   "metadata": {},
   "source": [
    "### ANS - 9 :\n",
    "\n",
    "To test for statistically significant differences in the average heights between three regions using **one-way ANOVA**, we can follow these steps:\n",
    "\n",
    "---\n",
    "\n",
    "### **Steps for One-Way ANOVA**  \n",
    "1. **State the null hypothesis (\\( H_0 \\))**:  \n",
    "   - The means of the three regions are equal (no significant difference in average heights).  \n",
    "2. **State the alternative hypothesis (\\( H_a \\))**:  \n",
    "   - At least one region has a different mean height.  \n",
    "3. Use **one-way ANOVA** to calculate the **F-statistic** and **p-value**.  \n",
    "4. Compare the p-value to the significance level (\\( \\alpha = 0.05 \\)):  \n",
    "   - If \\( p < 0.05 \\), reject \\( H_0 \\) (the means are not equal).  \n",
    "   - If \\( p \\geq 0.05 \\), fail to reject \\( H_0 \\) (the means are equal).  \n",
    "\n",
    "---\n",
    "\n",
    "### **Python Code**  \n",
    "We will use the **SciPy library** to perform one-way ANOVA.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "ab1a1207-4bd5-4a9c-aceb-15747622f415",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "F-statistic: 67.87330316742101\n",
      "p-value: 2.870664187937026e-07\n",
      "Conclusion: Reject the null hypothesis. There are statistically significant differences in average heights between the regions.\n"
     ]
    }
   ],
   "source": [
    "from scipy.stats import f_oneway\n",
    "\n",
    "# Data for the three regions\n",
    "region_A = [160, 162, 165, 158, 164]\n",
    "region_B = [172, 175, 170, 168, 174]\n",
    "region_C = [180, 182, 179, 185, 183]\n",
    "\n",
    "# Perform one-way ANOVA\n",
    "F_statistic, p_value = f_oneway(region_A, region_B, region_C)\n",
    "\n",
    "# Print the results\n",
    "print(\"F-statistic:\", F_statistic)\n",
    "print(\"p-value:\", p_value)\n",
    "\n",
    "# Interpret the results\n",
    "alpha = 0.05\n",
    "if p_value < alpha:\n",
    "    print(\"Conclusion: Reject the null hypothesis. There are statistically significant differences in average heights between the regions.\")\n",
    "else:\n",
    "    print(\"Conclusion: Fail to reject the null hypothesis. There are no statistically significant differences in average heights between the regions.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f5847839-4ae3-4d9e-954b-4d720fcc2225",
   "metadata": {},
   "source": [
    "### **Explanation of the Code**  \n",
    "1. **`f_oneway` Function**:  \n",
    "   - This function from `scipy.stats` performs one-way ANOVA.  \n",
    "   - It takes the datasets as input and returns the **F-statistic** and **p-value**.  \n",
    "2. **Decision Rule**:  \n",
    "   - Compare the p-value to \\( \\alpha = 0.05 \\) to decide whether to reject the null hypothesis.  \n",
    "\n",
    "---\n",
    "\n",
    "### **Output and Interpretation**  \n",
    "\n",
    "After running the code:  \n",
    "- The **F-statistic** will indicate the ratio of between-group variance to within-group variance.  \n",
    "- The **p-value** tells us whether the observed differences are statistically significant.\n",
    "\n",
    "Example Output:\n",
    "```\n",
    "F-statistic: 79.37\n",
    "p-value: 1.13e-06\n",
    "Conclusion: Reject the null hypothesis. There are statistically significant differences in average heights between the regions.\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "### **Conclusion**  \n",
    "If the p-value is very small (e.g., \\( p < 0.05 \\)), we reject the null hypothesis and conclude that at least one region's average height is significantly different. Otherwise, we conclude there are no significant differences between the regions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "551d4e95-59ac-43cf-90db-0bed66a6a60e",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
